{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONMOAXihJVvCVXSlzCG+Ut"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AntiPhish-LLM\n",
        "\n",
        "This is AntiPhish-LLM, a tool that leverages OpenAI's GPT-4 for the automatic classification of phishing emails and generation of warning messages.\n",
        "\n",
        "This tool was used in the study \"Can LLMs help protect users from phishing attacks? An exploratory study\", submitted for the CHI'24 conference, Late-Breaking Work track."
      ],
      "metadata": {
        "id": "ml9JrxW5xrZ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "Firstly, we install the needed python libraries."
      ],
      "metadata": {
        "id": "GIIAa4RCDLa4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlsMoDEYUoPo",
        "outputId": "dcdbb977-2136-4258-fd5e-0ec44bbbd077"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.23.5)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (8.2.3)\n",
            "Collecting cohere\n",
            "  Downloading cohere-4.39-py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.7/51.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (3.9.1)\n",
            "Collecting backoff<3.0,>=2.0 (from cohere)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting fastavro<2.0,>=1.8 (from cohere)\n",
            "  Downloading fastavro-1.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib_metadata<7.0,>=6.0 (from cohere)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.31.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.0.7)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (4.0.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (2023.11.17)\n",
            "Installing collected packages: importlib_metadata, fastavro, backoff, cohere\n",
            "  Attempting uninstall: importlib_metadata\n",
            "    Found existing installation: importlib-metadata 7.0.0\n",
            "    Uninstalling importlib-metadata-7.0.0:\n",
            "      Successfully uninstalled importlib-metadata-7.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 cohere-4.39 fastavro-1.9.1 importlib_metadata-6.11.0\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.2\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting openai\n",
            "  Downloading openai-1.5.0-py3-none-any.whl (223 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.7/223.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 openai-1.5.0\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.11.17)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Collecting dnspython\n",
            "  Downloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dnspython\n",
            "Successfully installed dnspython-2.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install scipy\n",
        "!pip install tenacity\n",
        "!pip install cohere\n",
        "!pip install tiktoken\n",
        "!pip install termcolor\n",
        "!pip install openai\n",
        "!pip install requests\n",
        "!pip install evals\n",
        "!pip install beautifulsoup4\n",
        "!pip install dnspython"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then define some utility functions."
      ],
      "metadata": {
        "id": "CYntLfQDDR_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import openai\n",
        "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
        "from termcolor import colored\n",
        "\n",
        "GPT_MODEL = \"gpt-4-1106-preview\" # \"gpt-3.5-turbo-0613\"\n",
        "\n",
        "# Replace with your OpenAI API key\n",
        "api_key = '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'\n",
        "\n",
        "# Set the API key\n",
        "openai.api_key = api_key\n",
        "\n",
        "# Utility functions definition\n",
        "\n",
        "@retry(wait=wait_random_exponential(multiplier=1, max=40), stop=stop_after_attempt(3))\n",
        "def chat_completion_request(messages, functions=None, function_call=None, model=\"gpt-3.5-turbo\", temperature=0, n_choices=1):\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": \"Bearer \" + openai.api_key,\n",
        "    }\n",
        "    json_data = {\"model\": model, \"temperature\": temperature, \"messages\": messages, \"n\": n_choices}\n",
        "    if functions is not None:\n",
        "        json_data.update({\"functions\": functions})\n",
        "    if function_call is not None:\n",
        "        json_data.update({\"function_call\": function_call})\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            \"https://api.openai.com/v1/chat/completions\",\n",
        "            headers=headers,\n",
        "            json=json_data,\n",
        "        )\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        print(\"Unable to generate ChatCompletion response\")\n",
        "        print(f\"Exception: {e}\")\n",
        "        return e\n",
        "\n",
        "\n",
        "def pretty_print_conversation(messages):\n",
        "    role_to_color = {\n",
        "        \"system\": \"red\",\n",
        "        \"user\": \"green\",\n",
        "        \"assistant\": \"blue\",\n",
        "        \"function\": \"magenta\",\n",
        "    }\n",
        "    for message in messages:\n",
        "        if message[\"role\"] == \"system\":\n",
        "            print(colored(f\"system: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
        "        elif message[\"role\"] == \"user\":\n",
        "            print(colored(f\"user: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
        "        elif message[\"role\"] == \"assistant\" and message.get(\"function_call\"):\n",
        "            print(colored(f\"assistant: {message['function_call']}\\n\", role_to_color[message[\"role\"]]))\n",
        "        elif message[\"role\"] == \"assistant\" and not message.get(\"function_call\"):\n",
        "            print(colored(f\"assistant: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
        "        elif message[\"role\"] == \"function\":\n",
        "            print(colored(f\"function ({message['name']}): {message['content']}\\n\", role_to_color[message[\"role\"]]))\n"
      ],
      "metadata": {
        "id": "m0YuKQWhVF7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Email Preprocessing and URL info gathering\n",
        "\n",
        "We define hereafter the function for preprocessing the email.\n",
        "\n",
        "We pre-process the email following the approach used in [K. Misra and J. T. Rayz, \"LMs go Phishing: Adapting Pre-trained Language Models to Detect Phishing Emails,\" 2022 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT), Niagara Falls, ON, Canada, 2022, pp. 135-142, doi: 10.1109/WI-IAT55865.2022.00028.](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10101955)"
      ],
      "metadata": {
        "id": "8sno3lHIxTXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import urllib.parse\n",
        "from bs4 import BeautifulSoup\n",
        "import quopri\n",
        "import email\n",
        "\n",
        "# Emails pre-processing\n",
        "\n",
        "def get_fullhostname(url):\n",
        "    # get the domain name from the URL (without the path)\n",
        "    re_match = re.search(r\"^(\\w+:\\/\\/)?(?:[^@\\/\\n]+@)?((?:www\\.)?[^:\\/?\\n]+)\\:?(\\d*)(\\/[^?]*)?(\\?.*)?\", url, re.I)\n",
        "    if (re_match != None):\n",
        "      re_match = re_match.groups()\n",
        "      full_hostname = re_match[0] + re_match[1]\n",
        "    return full_hostname\n",
        "\n",
        "def preprocess_email(email_content):\n",
        "  # Parse the email content\n",
        "  parser = email.parser.BytesParser()\n",
        "  email_message = parser.parsebytes(email_content)\n",
        "\n",
        "  ## Extract the subject\n",
        "  subject = email_message['subject'] or \"NO SUBJECT\"\n",
        "\n",
        "  ## Extract the email headers\n",
        "  headers = email_message.items()\n",
        "  # Convert the headers to a string\n",
        "  header_string = \"\\n\".join([f\"{key}: {value}\" for key, value in headers])\n",
        "\n",
        "  ## Extract the email body\n",
        "  body = \"\"\n",
        "  if email_message.is_multipart():\n",
        "      # If the email has multiple parts (e.g., text and HTML), we iterate through them\n",
        "      for part in email_message.walk():\n",
        "          content_type = part.get_content_type()\n",
        "          if content_type == 'text/plain' or content_type == 'text/html':\n",
        "              body += part.get_payload(decode=True).decode()\n",
        "  else:\n",
        "      # If the email is not multipart, it's a single plain text message\n",
        "      body = email_message.get_payload(decode=True).decode()\n",
        "\n",
        "  # Body pre-processing\n",
        "  urls_list = []\n",
        "  if body.find(\"Content-Transfer-Encoding: quoted-printable\") != -1:\n",
        "    print (\"Quoted-printable content\")\n",
        "    decoded_bytes_object = quopri.decodestring(body)\n",
        "    body = decoded_bytes_object.decode(\"utf-8\", errors=\"ignore\")  # TODO: get the right charset\n",
        "  soup = BeautifulSoup(body, 'html.parser')\n",
        "\n",
        "  for a_tag in soup.find_all(re.compile('a|img|div', re.I)):  # we try to find URLS in the href attribute of a, img, and div tags\n",
        "    href = a_tag.get(\"href\")\n",
        "    if href != None:\n",
        "      if href.startswith('tel') or href.startswith('sms'):\n",
        "        metatag = \"PHONE\"\n",
        "        href = href.replace(r'(tel|sms):', '')\n",
        "      elif href.startswith('mailto'):\n",
        "        metatag = \"EMAIL\"\n",
        "        href = href.replace('mailto:', '')\n",
        "      else:\n",
        "        metatag = \"URL\"\n",
        "        urls_list.append(href)\n",
        "      visible_string = a_tag.string or \"\"\n",
        "      a_tag.replace_with(f'[{metatag} HREF=\"{href}\"] {visible_string} [/{metatag}]')\n",
        "\n",
        "    body = soup.get_text()\n",
        "    # get the initial part of the URL only [protocol+FQDN(fully qualified domain name)] \\g<1> = protocol (+ www.), \\g<2> = FQDN\n",
        "    body = re.sub(r\"(https?:\\/\\/|www\\.)([-a-zA-Z0-9@:%._\\-\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6})\\b[-a-zA-Z0-9()@:%_+.~#?&\\/=\\-]*\", r\"\\g<1>\\g<2>\", body)\n",
        "\n",
        "    body = re.sub(r\" {2,}\", \" \", body)  # remove duplicate blanks\n",
        "    body = re.sub(r\"\\n{2,}\", \"\\n\", body) # remove duplicate \\n chars\n",
        "    #body = urllib.parse.parseqsl(body)\n",
        "    #body = body.replace(r'=[0-9A-F]{2}', '')\n",
        "\n",
        "  return {\n",
        "      \"headers\": headers,\n",
        "      \"subject\" : subject,\n",
        "      \"body\" : body,\n",
        "      \"urls\" : urls_list\n",
        "  }"
      ],
      "metadata": {
        "id": "hXW3jwdVvjwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## URL info\n",
        "\n",
        "We define the functions to collect online information about the URL(s) in the email."
      ],
      "metadata": {
        "id": "8-Gko4aEDeRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import base64\n",
        "import dns.resolver\n",
        "import socket\n",
        "\n",
        "vt_api_key = '02c720cb6e04487edb6384c18bb663da3667ca6e8f79925682d82e700f5ddae9' # VirusTotal API key\n",
        "blacklist_api_key = 'key_LS3loGrqCWUvKHRaCJgd9LiSR'  # Blacklist Checker API key\n",
        "dns_api_key = 'bdc_cd8f72a03c1843d59d402d7cdd1b0a6b' # BigDataCloud API key\n",
        "\n",
        "def get_ip_addr(url):\n",
        "    # remove protocol from URL (if exists)\n",
        "    match_result = re.match(r\"(?:.*\\:\\/\\/)([^\\/]*)\", url)\n",
        "    if match_result is not None:\n",
        "      match_groups = match_result.groups()\n",
        "      if len(match_groups) > 0:\n",
        "        url = match_groups[0]\n",
        "    # dns lookup\n",
        "    a = dns.resolver.resolve_name(url, family=socket.AF_INET)\n",
        "    addrs = a.addresses()\n",
        "    for addr in addrs:\n",
        "      return addr\n",
        "\n",
        "## VirusTotal\n",
        "def get_virustotal_data(url):\n",
        "    api_base_url = 'https://www.virustotal.com/api/v3/urls/'\n",
        "    # Headers with the API key\n",
        "    headers = {\n",
        "        'x-apikey': vt_api_key,\n",
        "    }\n",
        "    base_64_url = base64.b64encode(url.encode('ascii')) # encode the url in base64 bytes\n",
        "    base_64_url = base_64_url.decode(\"ascii\")  # get the base64 string\n",
        "    base_64_url = base_64_url.rstrip('=')  # remove the trailing padding chars '='\n",
        "    request_url = api_base_url + base_64_url\n",
        "\n",
        "    response = requests.get(request_url, headers=headers) # Make the HTTP GET request\n",
        "\n",
        "    # Check for a successful response (HTTP status code 200)\n",
        "    if response.status_code == 200:\n",
        "        # Access the JSON response\n",
        "        result = response.json()\n",
        "    else:\n",
        "        print(f\"VirusTotal Request failed with status code: {response.status_code}\")\n",
        "    vt_data = result['data']['attributes']['last_analysis_stats']  # contains the votes for the scan\n",
        "\n",
        "    response.close() # Close the response\n",
        "    return vt_data\n",
        "\n",
        "\n",
        "## Blackist Checker API\n",
        "def get_blacklists_data(url):\n",
        "    api_base_url = \"https://api.blacklistchecker.com/\"\n",
        "    headers = {\n",
        "        'authorization': \"Basic username\" + blacklist_api_key\n",
        "    }\n",
        "    request_url = api_base_url + \"check/\" + url\n",
        "    response = requests.get(request_url, headers=headers) # Make the HTTP GET request\n",
        "\n",
        "    # Check for a successful response (HTTP status code 200)\n",
        "    if response.status_code == 200:\n",
        "        # Access the JSON response\n",
        "        result = response.json()\n",
        "    else:\n",
        "        print(f\"BlacklistChecker Request failed with status code: {response.status_code}\")\n",
        "        result = {\"detections\": 0}\n",
        "    n_blacklists_found = result[\"detections\"]  # The detections field simply carries the number of blacklists in which the domain appeared\n",
        "\n",
        "    response.close() # Close the response\n",
        "    return n_blacklists_found\n",
        "\n",
        "\n",
        "## BigDataCloud API (get location of IP address)\n",
        "def get_dns_info(url):\n",
        "    api_base_url = \"https://api.bigdatacloud.net/data/country-by-ip\"\n",
        "    ip_addr = get_ip_addr(url)\n",
        "\n",
        "    get_params = {\n",
        "        \"ip\" : ip_addr,\n",
        "        \"key\" : dns_api_key\n",
        "    }\n",
        "    request_url = api_base_url\n",
        "    response = requests.get(request_url, params=get_params) # Make the HTTP GET request\n",
        "\n",
        "    # Check for a successful response (HTTP status code 200)\n",
        "    if response.status_code == 200:\n",
        "        # Access the JSON response\n",
        "        result = response.json()\n",
        "        # print (result)\n",
        "    else:\n",
        "        print(f\"BigDataCloud Request failed with status code: {response.status_code}\")\n",
        "        result = {\"country\": None}\n",
        "    country = result[\"country\"]\n",
        "    if country != None:\n",
        "       # countryName = country[\"name\"]  regionName = country[\"wbRegion\"][\"value\"]\n",
        "       countryID = country['isoAlpha3']  # country['isoName']\n",
        "    else:\n",
        "      countryID = \"unknown\"\n",
        "      # countryName = \"unknown\"  regionName = \"unknown\"\n",
        "    response.close() # Close the response\n",
        "    return countryID # countryName, regionName\n",
        "\n",
        "\n",
        "def get_url_info(url, string_out=False):\n",
        "    url = get_fullhostname(url_to_analyze) # gets the full host name (protocol + fqdn), without the URL path\n",
        "    vt_data = get_virustotal_data(url)\n",
        "    n_blacklists_found = get_blacklists_data(url)\n",
        "    domain_location = get_dns_info(url)\n",
        "\n",
        "    url_info = {\n",
        "        'Server location' : domain_location,\n",
        "        'VirusTotal scan' : vt_data,\n",
        "        'Blacklists' : n_blacklists_found\n",
        "    }\n",
        "    if string_out:\n",
        "      return str(url_info)\n",
        "    else:\n",
        "      return url_info\n"
      ],
      "metadata": {
        "id": "Zgj1v-F-lvy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's finally open an email and preprocess it.\n",
        "For now, we gather only URL info for the first URL in the email"
      ],
      "metadata": {
        "id": "z3QWDTaQx2If"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open and preprocess an email\n",
        "email_filename = \"email_name.eml\" # ENTER THE EMAIL FILE NAME HERE\n",
        "with open(email_filename, \"rb\") as email_byes:\n",
        "  mail = email_byes.read()\n",
        "  mail = preprocess_email(mail)\n",
        "  # Print or use the extracted subject, header, and body as needed\n",
        "  \"\"\"print(\"Subject:\", mail[\"subject\"])\n",
        "  print(\"Headers:\")\n",
        "  print(mail[\"headers\"])\n",
        "  print(\"Body:\")\n",
        "  print(mail[\"body\"])\n",
        "  print(\"URLS:\")\n",
        "  print(mail[\"urls\"])\"\"\"\n",
        "# Gather additional information about URLs in the email\n",
        "if len(mail[\"urls\"]) > 0:\n",
        "  # Call remote API to gather online URL information\n",
        "  url_to_analyze = mail[\"urls\"][0]  # for now we take the first URL\n",
        "  url_info = get_url_info(url_to_analyze);"
      ],
      "metadata": {
        "id": "7h3TUlYaluO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classify the email with GPT-3.5"
      ],
      "metadata": {
        "id": "gRQ7WP6rCwNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have our preprocessed email and URL information, we call GPT-3.5 for the classification.\n",
        "\n",
        "We already did manual prompt engineering and came up with the following prompt:\n",
        "\n",
        "\n",
        "```\n",
        "You are a cybersecurity expert that has the goal to detect if an email is legitimate or phishing and help the user understand the decision.\n",
        "The email is accompanied by information of the URLs in the email like: server location, in how many blacklists the site was found, VirusTotal scans reporting the number of scanners that detected the URL as harmless, undetected, suspicious, malicious.\n",
        "Your goal is to output a percentage indicating the probability of the email being phishing (0%=email is surely legitimate, 100%=email is surely phishing). Specifically, you should consider if and how many persuasion principles are applied by the alliged attacker; for each principle, you should report the part of the email that makes you think that persuasion principle is being applied; also add a brief rationale for each.\n",
        "Moreover, in the cases where the email is suspicious, you have to report 3 to 5 features that could indicate the danger in the email understandable by users with no cybersecurity or computers expertise\n",
        "Desired format:\n",
        "Label: [phishing/legit]\n",
        "Phishing Probability: [0-100%]\n",
        "Persuasion Principles found: <bullet list of persuation principles + specific sentences + rationale>\n",
        "Explanation: <bullet list of 3-5 features explained>\n",
        "```\n",
        "\n",
        "Let's modify it a bit and set the code for the API call using Chat Completions ([source](https://platform.openai.com/docs/guides/gpt/chat-completions-api)).\n",
        "\n",
        "## Generating a warning message with explanation\n",
        "\n",
        "Now we have a classification for an email that also carries a lot of\n",
        "information regarding the rationale for the classification and the persuasion principles that might have been used.\n",
        "\n",
        "Nonetheless, we want to have an explanation message that would be easy to understand also by lay users. Therefore, we create another prompt to further refine this longer explanation in an effective warning message.\n",
        "\n",
        "We created this prompt:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Now take the most relevant feature among the ones in your explanations and construct a brief explanation message (max 50 words) directed to naive users (with no knowledge of cybersecurity) that will follow this structure:`\n",
        "1. description of the most relevant phishing feature\n",
        "2. explanation of the hazard\n",
        "3. consequences of a successful phishing attack\n",
        "For example, an explanation that explains that the top-level domain in one of the email's URL is mispositioned would be:\n",
        "\"In the URL present in the email the top-level domain is in an abnormal position. This could indicate that the URL leads to a fake website. Such websites might steal your personal information”.\n",
        "Another example of explanation about the domain of a website being suspiciously young would be:\n",
        "\"The URL in the email leads to a website created N days ago. Young websites are famous for criminal activity. There is a potential risk if you proceed.\"\n",
        "Another example of explaining that the email is suspicious because of too many special characters in its body would be:\n",
        "\"Many special characters have been detected in the email. Malicious people use them to disguise text and deceive you. Your data could be stolen.\"\n",
        "\n",
        "\n",
        "Desired format:\n",
        "[description of the feature]. [hazard explanation]. [consequences of a successful attack].\n",
        "```\n",
        "\n",
        "https://www.notion.so/Progress-2b58877c7476447db56f582bacca6ed2\n",
        "\n"
      ],
      "metadata": {
        "id": "KDu6JIRHtZeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "MODEL =  \"gpt-4-1106-preview\" # \"gpt-3.5-turbo-1106\"\n",
        "TEMPERATURE = 0\n",
        "# TODO: see if we can look at the probability of generating the \"phishing\" or \"legit\" token\n",
        "\n",
        "def classify_email(email_input, feature_to_explain=None, url_info=None, explanations_min=3, explanations_max=6, model=MODEL):\n",
        "    # Initial Prompt\n",
        "    messages = [{\"role\": \"system\", \"content\": f'''You are a cybersecurity and human-computer interaction expert that has the goal to detect\n",
        "        if an email is legitimate or phishing and help the user understand why a specific email is dangerous (or genuine), in order\n",
        "        to make more informed decisions.\n",
        "        The user will submit the email (headers + subject + body) optionally accompanied by information of the URLs in the email as:\n",
        "        - server location;\n",
        "        - VirusTotal scans reporting the number of scanners that detected the URL as harmless, undetected, suspicious, malicious;\n",
        "        - number of blacklists in which the linked domain was found.\n",
        "\n",
        "        Your goal is to output a JSON object containing:\n",
        "        - The classification result (label).\n",
        "        - The probability in percentage of the email being phishing (0%=email is surely legitimate, 100%=email is surely phishing) (phishing_probability).\n",
        "        - A list of persuasion principles that were applied by the alliged attacker (if any); each persuasion principle should be an object containing:\n",
        "            the persuasion principle name (authority, scarcity, etc.),\n",
        "            the part of the email that makes you say that persuasion principle is being applied;\n",
        "            a brief rationale for each principle.\n",
        "        - A list of {explanations_min} to {explanations_max} features that could indicate the danger (or legitimacy) of the email; the explanations must be understandable by users with no cybersecurity or computers expertise.\n",
        "\n",
        "\n",
        "        Desired format:\n",
        "        label: <phishing/legit>\n",
        "        phishing_probability: <0-100%>\n",
        "        persuasion_principles: [array of persuation principles, each having: {{name, specific sentences, rationale}} ]\n",
        "        explanation: [array of {explanations_min}-{explanations_max} features explained]'''\n",
        "      }]\n",
        "    # User input (email)\n",
        "    headers = str(email_input[\"headers\"])\n",
        "    subject = email_input[\"subject\"]\n",
        "    body = email_input[\"body\"]\n",
        "    email_prompt = f'''Email:\n",
        "          \"\"\"\n",
        "          [HEADERS]\n",
        "            {headers}\n",
        "          [\\HEADERS]\n",
        "          [SUBJECT] {subject} [\\SUBJECT]\n",
        "          [BODY]\n",
        "          {body}\n",
        "          [\\BODY]\n",
        "          \"\"\"\n",
        "          '''\n",
        "    # Add the url_info if it exists\n",
        "    if url_info is not None:\n",
        "      email_prompt += f\"\"\"\n",
        "\n",
        "          ######\n",
        "\n",
        "          URL Information:\n",
        "          {str(url_info)}\"\"\"\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": email_prompt})\n",
        "    # Get the classification response\n",
        "    response = openai.chat.completions.create(\n",
        "      model=MODEL,\n",
        "      seed=SEED,\n",
        "      temperature=TEMPERATURE,\n",
        "      messages=messages,\n",
        "      response_format = {\"type\": \"json_object\"}\n",
        "    )\n",
        "    classification_response = response.choices[0].message.content\n",
        "\n",
        "    messages.append({\"role\": \"assistant\", \"content\": f\"{classification_response}\"}) # attach the response string for the second prompt\n",
        "\n",
        "    # Try getting the JSON object from the response\n",
        "    try:\n",
        "      classification_response = json.loads(classification_response)\n",
        "    except:\n",
        "      print (\"Invalid JSON format in the response\")\n",
        "      return classification_response, \"\"\n",
        "\n",
        "\n",
        "    if \"label\" in classification_response:\n",
        "      predicted_label = classification_response['label']\n",
        "      if predicted_label == \"legit\":\n",
        "        # If the classification == legit, then exit the function\n",
        "        return classification_response, \"The email is genuine\"\n",
        "      else:\n",
        "        # Otherwise, we ask GPT to produce the warning message\n",
        "        if feature_to_explain == None:\n",
        "          # Automatically take the most relevant feature\n",
        "          messages.append(\n",
        "              {\"role\": \"user\", \"content\": \"\"\"\n",
        "              Now take the most relevant feature among the ones in your explanations and construct a brief explanation message (max 50 words) directed to naive users (with no knowledge of cybersecurity) that will follow this structure:`\n",
        "              1. description of the most relevant phishing feature\n",
        "              2. explanation of the hazard\n",
        "              3. consequences of a successful phishing attack\n",
        "              For example, an explanation that explains that the top-level domain in one of the email's URL is mispositioned would be:\n",
        "              \"In the URL present in the email the top-level domain is in an abnormal position. This could\n",
        "              indicate that the URL leads to a fake website. Such websites might steal your personal\n",
        "              information”.\n",
        "              Another example of explanation about the domain of a website being suspiciously young would be:\n",
        "              \"The URL in the email leads to a website created N days ago. Young websites are famous for criminal activity. There is a potential risk if you proceed.\"\n",
        "              Another example of explaining that the email is suspicious because of too many special characters in its body would be:\n",
        "              \"Many special characters have been detected in the email. Malicious people use them to disguise text and deceive you. Your data could be stolen.\"\n",
        "\n",
        "              Desired format:\n",
        "              [description of the feature]. [hazard explanation]. [consequences of a successful attack].\n",
        "              \"\"\"}\n",
        "            )\n",
        "        else:\n",
        "          # Be primed about the feature to explain\n",
        "          messages.append(\n",
        "              {\"role\": \"user\", \"content\": f\"\"\"\n",
        "              Consider that the previous email is suspicious because {feature_to_explain[\"description\"]}: construct a brief explanation message (max 50 words) directed to naive users (with no knowledge of cybersecurity) that will follow this structure:`\n",
        "              1. description of the feature (in this case {feature_to_explain[\"name\"]})\n",
        "              2. explanation of the hazard\n",
        "              3. consequences of a successful phishing attack\n",
        "              For example, an explanation that explains that the top-level domain in one of the email's URL is mispositioned would be:\n",
        "              \"In the URL present in the email the top-level domain is in an abnormal position. This could\n",
        "              indicate that the URL leads to a fake website. Such websites might steal your personal\n",
        "              information\".\n",
        "              Another example of explanation about the domain of a website being suspiciously young would be:\n",
        "              \"The URL in the email leads to a website created N days ago. Young websites are famous for criminal activity. There is a potential risk if you proceed.\"\n",
        "              Another example of explaining that the email is suspicious because of too many special characters in its body would be:\n",
        "              \"Many special characters have been detected in the email. Malicious people use them to disguise text and deceive you. Your data could be stolen.\"\n",
        "\n",
        "              Desired format:\n",
        "              [description of the feature]. [hazard explanation]. [consequences of a successful attack].\n",
        "              \"\"\"}\n",
        "            )\n",
        "\n",
        "\n",
        "        response_2 = openai.chat.completions.create(\n",
        "          model=MODEL,\n",
        "          seed=SEED,\n",
        "          temperature=TEMPERATURE,\n",
        "          messages=messages\n",
        "        )\n",
        "        explanation_response = response_2.choices[0].message.content\n",
        "      return classification_response, explanation_response\n",
        "    else: # Error: response in wrong format\n",
        "      print (\"The response does not contain the predicted label (phishing/non-phishing)\")\n",
        "      return classification_response, \"\"\n"
      ],
      "metadata": {
        "id": "RmVHAoiq_x7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's generate an email classification + explanation:"
      ],
      "metadata": {
        "id": "_l76kI9gEq8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call GPT-3.5-turbo for email phishing classification (automatic feature detection)\n",
        "classification_response_3dot5, warning_msg_3dot5 = classify_email(mail, url_info, model=\"gpt-3.5-turbo-1106\") #=MODEL)\n",
        "\n",
        "print (classification_response_3dot5)\n",
        "print (warning_msg_3dot5)"
      ],
      "metadata": {
        "id": "LGr0jWFGEl4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Young domain\n",
        "# Open and preprocess the email\n",
        "email_filename = \"phishing_BRT.eml\"\n",
        "with open(email_filename, \"rb\") as email_byes:\n",
        "  mail = email_byes.read()\n",
        "  mail = preprocess_email(mail)\n",
        "# Call GPT for email phishing classification (priming the model about the feature to explain)\n",
        "feature_to_explain = {\n",
        "    \"name\" : 'Young domain',\n",
        "    \"description\" : 'it contains an URL that leads to a domain that is very new'\n",
        "}\n",
        "classification_response, warning_msg = classify_email(mail, feature_to_explain=feature_to_explain, url_info=None, model=MODEL)\n",
        "\n",
        "print (classification_response)\n",
        "print (warning_msg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4tXyN9lu3iM",
        "outputId": "d8da12a6-088f-4eb8-9d47-fdfec050544f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'label': 'phishing', 'phishing_probability': '95%', 'persuasion_principles': [{'name': 'Authority', 'specific_sentences': 'BRT Pacchetto in attesa', 'rationale': 'The email impersonates an authoritative courier service to create a sense of legitimacy and urgency.'}, {'name': 'Scarcity', 'specific_sentences': 'Hai (1) pacco in attesa di consegna.', 'rationale': 'The message suggests that there is a limited time to act on the delivery, which can pressure the recipient into taking hasty actions without proper verification.'}, {'name': 'Consistency', 'specific_sentences': 'Pianifica la tua consegna e iscriviti ai nostri avvisi di calendario per evitare che ciò accada di nuovo!!', 'rationale': 'The email prompts the user to take immediate action to resolve an issue, playing on the desire to maintain consistency and follow through with commitments, such as receiving a package.'}], 'explanation': [\"The sender's email address ('jessicahwhite47879@gmail.com') does not match the official email domain of a legitimate courier service, which is a common tactic used by phishers to deceive recipients.\", 'The email contains multiple grammatical and formatting errors, which are not expected in official communication from a reputable company.', 'The email creates a sense of urgency by claiming that a package is waiting for delivery, which is a tactic often used in phishing to prompt quick action without careful consideration.', \"The email includes suspicious links with unrelated URLs ('https://lnkd.in' and 'https://www.truilsjq.jbn'), which are likely not associated with the claimed courier service and could potentially lead to malicious websites.\", 'The email lacks personalization, using a generic greeting and not including specific details about the recipient or the package, which is atypical for legitimate parcel delivery notifications.', 'The email requests the recipient to confirm or take action by clicking on a link, which is a common phishing technique to lure individuals into providing sensitive information or downloading malware.']}\n",
            "The URL in the email leads to a very new domain. New domains are often used by scammers for fraud. You could be tricked into giving away personal details or downloading harmful software.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IP Address\n",
        "# Open and preprocess the email\n",
        "email_filename = \"phishing_IP.eml\"\n",
        "with open(email_filename, \"rb\") as email_byes:\n",
        "  mail = email_byes.read()\n",
        "  mail = preprocess_email(mail)\n",
        "# Call GPT for email phishing classification (priming the model about the feature to explain)\n",
        "feature_to_explain = {\n",
        "    \"name\" : \"URL is IP address\",\n",
        "    \"description\" : \"it contains an URL that is an IP address\"\n",
        "}\n",
        "classification_response, warning_msg = classify_email(mail, feature_to_explain=feature_to_explain, url_info=None, model=MODEL)\n",
        "\n",
        "print (classification_response)\n",
        "print (warning_msg)"
      ],
      "metadata": {
        "id": "mqdE6XNmV_U4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TLD mispositioned\n",
        "# Open and preprocess the email\n",
        "email_filename = \"phishing_TLD_mispositioned.eml\"\n",
        "with open(email_filename, \"rb\") as email_byes:\n",
        "  mail = email_byes.read()\n",
        "  mail = preprocess_email(mail)\n",
        "# Call GPT for email phishing classification (priming the model about the feature to explain)\n",
        "feature_to_explain = {\n",
        "    \"name\" : \"Top-Level Domain mispositioned\",\n",
        "    \"description\" : \"it contains an URL with a top-level domain (.com) found as a subdomain\"\n",
        "}\n",
        "classification_response, warning_msg = classify_email(mail, feature_to_explain=feature_to_explain, url_info=None, model=MODEL)\n",
        "\n",
        "print (classification_response)\n",
        "print (warning_msg)"
      ],
      "metadata": {
        "id": "812Nfu5nqYhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Link mismatch\n",
        "# Open and preprocess the email\n",
        "email_filename = \"phishing_link_mismatch.eml\"\n",
        "with open(email_filename, \"rb\") as email_byes:\n",
        "  mail = email_byes.read()\n",
        "  mail = preprocess_email(mail)\n",
        "# Call GPT for email phishing classification (priming the model about the feature to explain)\n",
        "feature_to_explain = {\n",
        "    \"name\" : \"Link mismatch\",\n",
        "    \"description\" : \"it contains a displayed link that is different from the actual pointed URL\"\n",
        "}\n",
        "classification_response, warning_msg = classify_email(mail, feature_to_explain=feature_to_explain, url_info=None, model=MODEL)\n",
        "\n",
        "print (classification_response)\n",
        "print (warning_msg)"
      ],
      "metadata": {
        "id": "xdTIfIh4stD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call GPT-4-turbo for email phishing classification\n",
        "classification_response_4, warning_msg_4 = classify_email(mail, url_info, model=\"gpt-4-1106-preview\")\n",
        "\n",
        "print (classification_response_4)\n",
        "print (warning_msg_4)"
      ],
      "metadata": {
        "id": "xrwgTRm-9jCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT-3.5:\n",
        "Example of a wrong classification (false positive) - we must acknowledge that it classify it correctly the second time we ran the prompt:\n",
        "```\n",
        "{\n",
        "  'label': 'phishing',\n",
        "  'probability': '95%',\n",
        "  'persuasion_principles':\n",
        "  [\n",
        "    'Authority: The email uses the name of ngrok founder and CEO Alan Shreve to establish credibility and persuade the recipient to click on the links.',\n",
        "    \"Scarcity: The email mentions 'Pay-as-you-go pricing' and 'Secure your ngrok account with MFA' to create a sense of urgency and persuade the recipient to take immediate action.\",\n",
        "    'Social Proof: The email includes links to GitHub, LinkedIn, Twitter, and YouTube to show social validation and persuade the recipient to trust the content.'\n",
        "  ],\n",
        "  'explanation':\n",
        "  [\n",
        "    'Unsolicited URLs: The email contains multiple unsolicited URLs, which is a common tactic used in phishing emails to lure recipients to malicious websites.',\n",
        "    \"Urgent Call to Action: The email creates a sense of urgency by promoting 'Pay-as-you-go pricing' and 'Secure your ngrok account with MFA', which is a typical strategy used in phishing emails to prompt immediate action.\",\n",
        "    \"Unverified Sender: The email claims to be from 'ngrok', but the sender's address is from a suspicious domain '21124867m.ngrok-info.com', indicating a potential impersonation attempt.\"\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "Warning message:\n",
        "**The email contains multiple unsolicited URLs, a common tactic used in phishing emails to lure recipients to malicious websites. Clicking on these links could lead to fake websites designed to steal personal and financial information, potentially resulting in identity theft or financial loss.**\n",
        "\n",
        "\n",
        "### GPT-4:\n",
        "\n",
        "Same email, correct classification\n",
        "\n",
        "```\n",
        "{'label': 'legit',\n",
        " 'probability': '0%',\n",
        " 'persuasion_principles': [],\n",
        " 'explanation': [\"The email passed SPF, DKIM, and DMARC checks, which are email authentication methods that help prevent email spoofing. The headers show 'spf=pass', 'dkim=pass', and 'dmarc=pass', indicating that the email was sent from an authorized server and the content has not been tampered with.\",\n",
        "  \"The sender's domain 'ngrok.com' is consistent with the content of the email, which discusses ngrok's services. This alignment between the sender and the content suggests that the email is legitimate.\",\n",
        "  'The email contains multiple URLs, but all of them point to the same domain, which is consistent with legitimate marketing practices where a company promotes its own content and services.',\n",
        "  'The VirusTotal scan reports that the URL was detected as harmless by 71 scanners and undetected by 18, with no scanners flagging it as suspicious or malicious. This indicates that the URL is safe to visit.',\n",
        "  'The linked domain was not found on any blacklists, which suggests that it is not associated with any known malicious activities.',\n",
        "  'The email provides a clear way to unsubscribe or manage preferences, which is a common practice in legitimate marketing emails to comply with anti-spam laws.']}\n",
        "  ```\n",
        "  \n",
        "No warning message - **The email is genuine**\n"
      ],
      "metadata": {
        "id": "JnyDP0eh7JDi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Future work (benchmarking GPT)\n",
        "Let's benchmark GPT-3.5-turbo's performance on a dataset of phishing/legitimate emails. If we are not satisfied with the performance, we can consider fine-tuning the model next."
      ],
      "metadata": {
        "id": "BFwxe8qOztJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate GPT's outputs without fine-tuning https://github.com/openai/evals\n",
        "\n",
        "## Gather the email dataset\n",
        "\n",
        "# Pre-process the emails\n",
        "\n",
        "# For each email, retrieve URL information\n",
        "\n",
        "# Use OpenAI Evals framework to evaluate GPT's performance on our dataset\n"
      ],
      "metadata": {
        "id": "YsfnP-dFm9Kp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}